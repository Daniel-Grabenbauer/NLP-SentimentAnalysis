{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# global vars for local system\n",
    "BASEPATH = r\"C:\\Users\\sdola\\Nextcloud\\Technikum\\NLP\\Project\"\n",
    "TRACKER_FILE = rf\"{BASEPATH}\\derstandard_tracker.txt\"\n",
    "SQLITE_DATABASE = rf\"{BASEPATH}\\derstandard.db\"\n",
    "CHROMEDRIVER_PATH = r\"C:\\Users\\sdola\\Documents\\chromedriver-win64\\chromedriver.exe\"\n",
    "FRONTPAGE_URL = \"https://www.derstandard.at/frontpage/\"\n",
    "\n",
    "\n",
    "def get_most_recent_download():\n",
    "    try:\n",
    "        with open(TRACKER_FILE, mode=\"r\") as f:\n",
    "            line = f.readlines()[-1]\n",
    "            dl_dt = line.split(\"\\t\")[-1].replace(\"\\n\", \"\")\n",
    "            return datetime.datetime.strptime(dl_dt, \"%Y-%m-%d\").date()\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "\n",
    "def create_database_and_tables():\n",
    "    conn = sqlite3.connect(SQLITE_DATABASE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS urls (\n",
    "        URL_ID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        URL TEXT UNIQUE,\n",
    "        publication_date DATE,\n",
    "        url_download_date DATE\n",
    "    )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def setup_driver(headless=True):\n",
    "    chrome_options = wd.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\")\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "    \n",
    "    chrome_options.page_load_strategy = 'none' \n",
    "    chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "\n",
    "    service = Service(executable_path=CHROMEDRIVER_PATH)\n",
    "    driver = wd.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # POPUP WEGKLICKEN\n",
    "    driver.get(FRONTPAGE_URL + datetime.datetime.today().strftime(\"%Y/%m/%d\"))\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            lambda driver: driver.execute_script(\"return document.readyState\") == 'complete'\n",
    "        )\n",
    "        driver.switch_to.frame(driver.find_element(By.XPATH, \"/html/body/div/iframe\"))\n",
    "        driver.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div[3]/div[1]/button\").click()\n",
    "        driver.switch_to.parent_frame()\n",
    "    except NoSuchElementException:\n",
    "        print(\"popup nicht gefunden\")\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def scrape_derstandard():\n",
    "\n",
    "    # prepare driver\n",
    "    start = (get_most_recent_download() + datetime.timedelta(days=1)) or datetime.date(1998, 11, 29)\n",
    "    driver = setup_driver()\n",
    "\n",
    "    # connect to db\n",
    "    conn = sqlite3.connect(SQLITE_DATABASE)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    while start < datetime.date.today():\n",
    "        archive_url = FRONTPAGE_URL + start.strftime(\"%Y/%m/%d\")\n",
    "        try:\n",
    "            driver.get(archive_url)\n",
    "        except TimeoutException:\n",
    "            driver.quit()\n",
    "            driver = setup_driver()\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, \"html5lib\")\n",
    "        for article in soup.findAll(\"article\"):\n",
    "            try:\n",
    "                url = article.find(\"a\").get(\"href\")\n",
    "                cursor.execute(\"INSERT INTO urls (URL, publication_date, url_download_date) VALUES (?, ?, ?)\", (url, start, None))\n",
    "            except sqlite3.IntegrityError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "            \n",
    "        print(f\"{start} erfolgreich gescraped.\" )\n",
    "        \n",
    "        with open(TRACKER_FILE, \"a\") as tracker_file:\n",
    "            tracker_file.write(f\"{start}\\n\")\n",
    "\n",
    "        start += datetime.timedelta(days=1)\n",
    "        \n",
    "    driver.quit()\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comments_table():\n",
    "    conn = sqlite3.connect('derstandard.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS comments (\n",
    "        commentID INTEGER PRIMARY KEY,\n",
    "        articleID INTEGER,\n",
    "        username TEXT,\n",
    "        datetime DATETIME,\n",
    "        comment_header TEXT,\n",
    "        comment TEXT,\n",
    "        upvotes INTEGER,\n",
    "        downvotes INTEGER,\n",
    "        user_followers INTEGER,\n",
    "        reply_on_comment INTEGER,\n",
    "        FOREIGN KEY(articleID) REFERENCES Articles(articleID),\n",
    "        FOREIGN KEY(reply_on_comment) REFERENCES comments(commentID)\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "create_comments_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_articles_for_party(conn, party, year, limit=10):\n",
    "    # Stellen Sie sicher, dass 'limit' ein Integer ist\n",
    "    limit = int(limit)\n",
    "\n",
    "    start_date = f'{year}-01-01'\n",
    "    end_date = f'{year}-12-31'\n",
    "\n",
    "    query = f'''\n",
    "    SELECT a.articleID, u.url\n",
    "    FROM Articles a\n",
    "    JOIN Urls u ON a.urlID = u.urlID\n",
    "    WHERE a.articleID IN (\n",
    "        -- Artikel mit genau einem Keyword\n",
    "        SELECT ak.articleID\n",
    "        FROM Article_Keywords ak\n",
    "        GROUP BY ak.articleID\n",
    "        HAVING COUNT(*) = 1\n",
    "    )\n",
    "    AND a.articleID IN (\n",
    "        -- Artikel, deren einziges Keyword die gewünschte Partei ist\n",
    "        SELECT ak.articleID\n",
    "        FROM Article_Keywords ak\n",
    "        JOIN Keywords k ON ak.keywordID = k.keywordID\n",
    "        WHERE k.keyword = ?\n",
    "    )\n",
    "    AND a.datetime BETWEEN ? AND ?\n",
    "    AND a.kicker != ?\n",
    "    LIMIT {limit}\n",
    "    '''\n",
    "\n",
    "    params = [party, start_date, end_date, 'Switchlist']\n",
    "    df = pd.read_sql_query(query, conn, params=params)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateparser\n",
    "\n",
    "def parse_comment_datetime(datetime_str):\n",
    "    return dateparser.parse(datetime_str, languages=['de'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_comments_for_article(driver, article_url, articleID):\n",
    "    comments_data = []\n",
    "    driver.get(article_url)\n",
    "    time.sleep(5)  # Warten, bis die Seite geladen ist\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    postings = soup.find_all('div', class_='posting', attrs={'data-postingid': True})\n",
    "\n",
    "    for posting in postings[:10]:  # Bis zu 10 Kommentare sammeln\n",
    "        try:\n",
    "            commentID = posting.get('data-postingid')\n",
    "            username = posting.get('data-communityname') or 'gelöschtes Profil'\n",
    "            reply_on_comment = posting.get('data-parentpostingid')\n",
    "            reply_on_comment = int(reply_on_comment) if reply_on_comment else None\n",
    "\n",
    "            # Datum und Uhrzeit des Kommentars extrahieren\n",
    "            datetime_tag = posting.find('span', class_='js-timestamp')\n",
    "            if datetime_tag and datetime_tag.text:\n",
    "                datetime_str = datetime_tag.text.strip()\n",
    "                datetime_obj = parse_comment_datetime(datetime_str)\n",
    "            else:\n",
    "                datetime_obj = None  # Oder ein Standardwert\n",
    "\n",
    "            # Kommentarüberschrift extrahieren\n",
    "            comment_header_tag = posting.find('h4', class_='upost-title')\n",
    "            comment_header = comment_header_tag.text.strip() if comment_header_tag else None\n",
    "\n",
    "            # Kommentartext extrahieren\n",
    "            comment_body = posting.find('div', class_='upost-text')\n",
    "            comment_text = comment_body.get_text(separator=' ', strip=True) if comment_body else None\n",
    "\n",
    "            # Upvotes extrahieren\n",
    "            upvotes_tag = posting.find('span', class_='js-ratings-positive-count')\n",
    "            upvotes = int(upvotes_tag.text.strip()) if upvotes_tag and upvotes_tag.text else 0\n",
    "\n",
    "            # Downvotes extrahieren\n",
    "            downvotes_tag = posting.find('span', class_='js-ratings-negative-count')\n",
    "            downvotes = int(downvotes_tag.text.strip()) if downvotes_tag and downvotes_tag.text else 0\n",
    "\n",
    "            # Anzahl der Follower des Nutzers extrahieren\n",
    "            user_followers_tag = posting.find('span', class_='upost-follower')\n",
    "            user_followers = int(user_followers_tag.text.strip()) if user_followers_tag and user_followers_tag.text else 0\n",
    "\n",
    "            comments_data.append({\n",
    "                'commentID': int(commentID),\n",
    "                'articleID': articleID,\n",
    "                'username': username,\n",
    "                'datetime': datetime_obj,\n",
    "                'comment_header': comment_header,\n",
    "                'comment': comment_text,\n",
    "                'upvotes': upvotes,\n",
    "                'downvotes': downvotes,\n",
    "                'user_followers': user_followers,\n",
    "                'reply_on_comment': reply_on_comment\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten eines Kommentars in Artikel {articleID}: {e}\")\n",
    "            continue \n",
    "\n",
    "    return comments_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_comments_to_db(conn, comments_data):\n",
    "    cursor = conn.cursor()\n",
    "    for comment in comments_data:\n",
    "        cursor.execute('''\n",
    "        INSERT OR IGNORE INTO comments (\n",
    "            commentID, articleID, username, datetime, comment_header,\n",
    "            comment, upvotes, downvotes, user_followers, reply_on_comment\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            comment['commentID'],\n",
    "            comment['articleID'],\n",
    "            comment['username'],\n",
    "            comment['datetime'],\n",
    "            comment['comment_header'],\n",
    "            comment['comment'],\n",
    "            comment['upvotes'],\n",
    "            comment['downvotes'],\n",
    "            comment['user_followers'],\n",
    "            comment['reply_on_comment']\n",
    "        ))\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_comments():\n",
    "    conn = sqlite3.connect('derstandard.db')\n",
    "    create_comments_table()\n",
    "    parties = ['ÖVP', 'SPÖ', 'FPÖ', 'Grüne', 'NEOS']\n",
    "    years = range(2015, datetime.datetime.now().year + 1)\n",
    "\n",
    "    driver = setup_driver(headless=True)\n",
    "\n",
    "    for party in parties:\n",
    "        for year in years:\n",
    "            print(f\"Scrape Kommentare für {party} im Jahr {year}\")\n",
    "            articles_df = select_articles_for_party(conn, party, year, limit=10)\n",
    "            for _, row in articles_df.iterrows():\n",
    "                articleID = row['articleID']\n",
    "                article_url = 'https://www.derstandard.at' + row['url']\n",
    "                comments_data = scrape_comments_for_article(driver, article_url, articleID)\n",
    "                if comments_data:\n",
    "                    insert_comments_to_db(conn, comments_data)\n",
    "                    print(f\"{len(comments_data)} Kommentare für Artikel {articleID} gesammelt\")\n",
    "                else:\n",
    "                    print(f\"Keine Kommentare gefunden für Artikel {articleID}\")\n",
    "\n",
    "    driver.quit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%time create_database_and_tables()\n",
    "%time create_comments_table()\n",
    "%time scrape_derstandard()\n",
    "%time scrape_comments()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
