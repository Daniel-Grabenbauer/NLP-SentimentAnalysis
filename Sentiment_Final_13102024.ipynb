{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Lade die Sentiment-Analyse Pipeline mit dem gewünschten Modell\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"oliverguhr/german-sentiment-bert\")\n",
    "tokenizer = sentiment_pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sdola\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def split_text_into_chunks(text, max_chunk_size=480):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        sentence_length = len(tokens)\n",
    "\n",
    "        if sentence_length > max_chunk_size:\n",
    "            # Splitte den langen Satz in kleinere Teile\n",
    "            split_indices = range(0, sentence_length, max_chunk_size)\n",
    "            for i in split_indices:\n",
    "                sub_tokens = tokens[i:i + max_chunk_size]\n",
    "                sub_chunk = tokenizer.decode(sub_tokens, clean_up_tokenization_spaces=True)\n",
    "                sub_length = len(sub_tokens)\n",
    "                chunks.append((sub_chunk, sub_length))\n",
    "        else:\n",
    "            if current_length + sentence_length > max_chunk_size:\n",
    "                # wenn die aktuelle länge des chunks mit dem neuen satz, die maximale länger überschreiten würde... \n",
    "                if current_chunk:\n",
    "                    # füge den aktuellen chunk der finalen chunkliste hinzu und der neue satz ist der erste des nächsten Chunks.\n",
    "                    chunks.append((current_chunk, current_length))\n",
    "                current_chunk = sentence\n",
    "                current_length = sentence_length\n",
    "            else:\n",
    "                # sonst erweitere den aktuellen Chunk um den neuen Satz..\n",
    "                if current_chunk:\n",
    "                    current_chunk += \" \" + sentence\n",
    "                else:\n",
    "                    current_chunk = sentence\n",
    "                current_length += sentence_length\n",
    "\n",
    "    # wenn am Ende der current Chunk noch nicht leer ist, dann füge ihn der finalen chunk-Liste hinzu\n",
    "    if current_chunk:\n",
    "        chunks.append((current_chunk, current_length))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def analyze_sentiment(text, max_chunk_size=480):\n",
    "    # Schritt 1: Text in Chunks aufteilen\n",
    "    chunks = split_text_into_chunks(text, max_chunk_size)\n",
    "\n",
    "    if not chunks:\n",
    "        return 0.0, 'neutral'  # Standardwert, falls kein Text vorhanden ist\n",
    "\n",
    "    # entpacke chunks in jeweils eine liste aus den Texten und eine liste aus der Chunklängen\n",
    "    chunk_texts, chunk_lengths = zip(*chunks)\n",
    "\n",
    "    # Schritt 2: Sentiment-Analyse durchführen\n",
    "    results = sentiment_pipeline(list(chunk_texts))\n",
    "\n",
    "    sentiments = []\n",
    "    weights = []\n",
    "\n",
    "    for result, length in zip(results, chunk_lengths):\n",
    "        label = result['label'].lower()\n",
    "        score = result['score']\n",
    "\n",
    "        # Mapping der Sentiment-Klassen auf numerische Werte\n",
    "        if label == 'negative':\n",
    "            sentiment_value = -1\n",
    "        elif label == 'neutral':\n",
    "            sentiment_value = 0\n",
    "        elif label == 'positive':\n",
    "            sentiment_value = 1\n",
    "        else:\n",
    "            sentiment_value = 0  # Standard auf neutral, falls unbekannt\n",
    "\n",
    "        # Berechne den gewichteten Sentiment-Score\n",
    "        sentiment_score = sentiment_value * score\n",
    "        sentiments.append(sentiment_score)\n",
    "        weights.append(length)\n",
    "\n",
    "    # Schritt 3: Gewichteten Durchschnitt des Sentiments berechnen\n",
    "    weighted_sum = sum(s * w for s, w in zip(sentiments, weights))\n",
    "    total_weights = sum(weights)\n",
    "    average_sentiment = weighted_sum / total_weights if total_weights != 0 else 0\n",
    "\n",
    "    # Schritt 4: Klassifiziere den durchschnittlichen Sentiment-Wert in die drei Klassen\n",
    "    if average_sentiment < -0.5:\n",
    "        sentiment_class = 'negative'\n",
    "    elif -0.5 <= average_sentiment < 0.5:\n",
    "        sentiment_class = 'neutral'\n",
    "    else:\n",
    "        sentiment_class = 'positive'\n",
    "\n",
    "    return average_sentiment, sentiment_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_table(conn):\n",
    "    \"\"\"\n",
    "    Erstellt die Tabelle Article_Sentiment, falls sie nicht existiert.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS Article_Sentiment (\n",
    "        articleID INTEGER PRIMARY KEY,\n",
    "        sentiment_value REAL,\n",
    "        sentiment_category TEXT,\n",
    "        FOREIGN KEY(articleID) REFERENCES Articles(articleID)\n",
    "    )\n",
    "    ''')\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store_sentiment(conn, df, batch_size=100):\n",
    "    \"\"\"\n",
    "    Verarbeitet den DataFrame in Batches, berechnet Sentiment und speichert die Ergebnisse in der Datenbank.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Iteriere über den DataFrame in Batches\n",
    "    for start in tqdm(range(0, len(df), batch_size), desc=\"Processing Batches\"):\n",
    "        end = start + batch_size\n",
    "        batch_df = df.iloc[start:end]\n",
    "\n",
    "        # Liste für die Ergebnisse\n",
    "        sentiment_results = []\n",
    "\n",
    "        for _, row in batch_df.iterrows():\n",
    "            article_id = row['articleID']\n",
    "            text = row['article_text']\n",
    "            if not isinstance(text, str) or not text.strip():\n",
    "                average_sentiment = np.nan\n",
    "                sentiment_class = 'NA'\n",
    "            else:\n",
    "                try:\n",
    "                    average_sentiment, sentiment_class = analyze_sentiment(text)\n",
    "                except Exception as e:\n",
    "                    print(f\"Fehler bei Artikel-ID {article_id}: {e}\")\n",
    "                    average_sentiment = np.nan\n",
    "                    sentiment_class = 'NA'\n",
    "\n",
    "            sentiment_results.append((article_id, average_sentiment, sentiment_class))\n",
    "\n",
    "        # Einfügen in die Datenbank\n",
    "        try:\n",
    "            cursor.executemany('''\n",
    "                INSERT OR REPLACE INTO Article_Sentiment (articleID, sentiment_value, sentiment_category)\n",
    "                VALUES (?, ?, ?)\n",
    "            ''', sentiment_results)\n",
    "            conn.commit()\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"SQLite Fehler beim Einfügen der Sentiment-Daten: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  17%|█▋        | 127/746 [1:28:52<5:03:06, 29.38s/it] Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Batches: 100%|██████████| 746/746 [8:53:14<00:00, 42.89s/it]    \n"
     ]
    }
   ],
   "source": [
    "# Verbindung zur SQLite-Datenbank herstellen\n",
    "conn = sqlite3.connect('derstandard.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Tabelle Article_Sentiment erstellen, falls sie nicht existiert\n",
    "create_sentiment_table(conn)\n",
    "\n",
    "# Liste der aktuell relevanten Parteien\n",
    "current_parties = ['ÖVP', 'FPÖ', 'NEOS', 'Grüne', 'SPÖ', 'KPÖ']\n",
    "\n",
    "# Platzhalter und params für Abfrage erstellen\n",
    "placeholders = ','.join(['?'] * len(current_parties))\n",
    "params = current_parties + ['2015-01-01', 'Switchlist']\n",
    "\n",
    "# SQL-Abfrage zum Abrufen der Artikel, die eine der aktuellen Parteien behandeln UND ab 2015 UND nicht Switchlist\n",
    "query = f'''\n",
    "SELECT DISTINCT a.*\n",
    "FROM Articles a\n",
    "JOIN Article_Keywords ak ON a.articleID = ak.articleID\n",
    "JOIN Keywords k ON ak.keywordID = k.keywordID\n",
    "WHERE k.keyword IN ({placeholders}) AND a.datetime > ? AND a.kicker != ?\n",
    "'''\n",
    "\n",
    "# Abfrage ausführen\n",
    "df = pd.read_sql_query(query, conn, params=params)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Sentiment-Analyse batchweise durchführen und Ergebnisse speichern\n",
    "process_and_store_sentiment(conn, df, batch_size=100)\n",
    "\n",
    "# Verbindung schließen\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
